{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeaa54b9-03bd-4001-89d7-7bf885f01c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class SequentialTFModel:\n",
    "    def __init__(self, input_dim):\n",
    "        self.layers = []\n",
    "        self.input_dim = input_dim\n",
    "        self.prev_dim = input_dim\n",
    "\n",
    "    def add(self, units, activation='relu'):\n",
    "        W = tf.Variable(tf.random.normal([self.prev_dim, units], stddev=0.1), trainable=True)\n",
    "        b = tf.Variable(tf.zeros([units]), trainable=True)\n",
    "        self.layers.append({'W': W, 'b': b, 'activation': activation})\n",
    "        self.prev_dim = units\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for layer in self.layers:\n",
    "            Z = tf.matmul(out, layer['W']) + layer['b']\n",
    "            if layer['activation'] == 'relu':\n",
    "                out = tf.nn.relu(Z)\n",
    "            elif layer['activation'] == 'sigmoid':\n",
    "                out = tf.nn.sigmoid(Z)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported activation\")\n",
    "        return out\n",
    "\n",
    "    def train(self, X, Y, epochs=100, lr=0.01, loss_fn='mse'):\n",
    "        optimizer = tf.optimizers.Adam(lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self.forward(X)\n",
    "                if loss_fn == 'mse':\n",
    "                    loss = tf.reduce_mean((predictions - Y) ** 2)\n",
    "                elif loss_fn == 'bce':\n",
    "                    loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(Y, predictions))\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported loss\")\n",
    "\n",
    "            variables = []\n",
    "            for layer in self.layers:\n",
    "                variables += [layer['W'], layer['b']]\n",
    "\n",
    "            grads = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97adc71-1c1b-4bba-ab7e-a8220ed15663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward function\n",
    "def autoencoder_forward(X):\n",
    "    encoded = encoder.forward(X)\n",
    "    decoded = decoder.forward(encoded)\n",
    "    return decoded\n",
    "\n",
    "# trainable variables\n",
    "def get_autoencoder_variables():\n",
    "    vars = []\n",
    "    for model in [encoder, decoder]:\n",
    "        for layer in model.layers:\n",
    "            vars += [layer['W'], layer['b']]\n",
    "    return vars\n",
    "\n",
    "# Train function\n",
    "def train_autoencoder(X, epochs=100, lr=0.01):\n",
    "    optimizer = tf.optimizers.Adam(lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = autoencoder_forward(X)\n",
    "            loss = tf.reduce_mean((reconstructed - X) ** 2)\n",
    "\n",
    "        vars = get_autoencoder_variables()\n",
    "        grads = tape.gradient(loss, vars)\n",
    "        optimizer.apply_gradients(zip(grads, vars))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1791baae-9de3-4b23-a4ff-53c6963f540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.0834\n",
      "Epoch 10, Loss: 0.0833\n",
      "Epoch 20, Loss: 0.0833\n",
      "Epoch 30, Loss: 0.0824\n",
      "Epoch 40, Loss: 0.0787\n",
      "Epoch 50, Loss: 0.0778\n",
      "Epoch 60, Loss: 0.0773\n",
      "Epoch 70, Loss: 0.0765\n",
      "Epoch 80, Loss: 0.0752\n",
      "Epoch 90, Loss: 0.0738\n"
     ]
    }
   ],
   "source": [
    "# Dummy data\n",
    "input_dim = 16\n",
    "np.random.seed(42)\n",
    "X_data = np.random.rand(1000, input_dim).astype(np.float32)\n",
    "\n",
    "# encoder\n",
    "encoder = SequentialTFModel(input_dim)\n",
    "encoder.add(12, activation='relu')\n",
    "encoder.add(8, activation='relu')\n",
    "encoder.add(4, activation='relu') \n",
    "\n",
    "# decoder \n",
    "decoder = SequentialTFModel(4)\n",
    "decoder.add(8, activation='relu')\n",
    "decoder.add(12, activation='relu')\n",
    "decoder.add(input_dim, activation='sigmoid')  \n",
    "\n",
    "X_tensor = tf.convert_to_tensor(X_data)\n",
    "\n",
    "# Train autoencoder\n",
    "train_autoencoder(X_tensor, epochs=100, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eea58b4-7b3b-45b2-b3db-4e4a18050649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " [[0.37454012 0.9507143  0.7319939  0.5986585  0.15601864 0.15599452\n",
      "  0.05808361 0.8661761  0.601115   0.7080726  0.02058449 0.96990985\n",
      "  0.83244264 0.21233912 0.18182497 0.1834045 ]\n",
      " [0.30424225 0.52475643 0.43194503 0.29122913 0.6118529  0.13949387\n",
      "  0.29214466 0.36636186 0.45606998 0.785176   0.19967379 0.5142344\n",
      "  0.59241456 0.04645041 0.60754484 0.17052412]\n",
      " [0.06505159 0.94888556 0.965632   0.80839735 0.30461377 0.09767211\n",
      "  0.684233   0.4401525  0.12203824 0.4951769  0.03438852 0.9093204\n",
      "  0.25877997 0.66252226 0.31171107 0.52006805]\n",
      " [0.54671025 0.18485446 0.96958464 0.77513283 0.93949896 0.89482737\n",
      "  0.5979     0.9218742  0.08849251 0.19598286 0.04522729 0.32533032\n",
      "  0.3886773  0.27134904 0.8287375  0.35675332]\n",
      " [0.2809345  0.54269606 0.14092423 0.802197   0.07455064 0.9868869\n",
      "  0.77224475 0.19871569 0.00552212 0.81546146 0.7068573  0.7290072\n",
      "  0.77127033 0.07404465 0.35846573 0.11586906]]\n",
      "Reconstructed:\n",
      " [[0.51007825 0.5003284  0.4438317  0.46769512 0.52557915 0.5406957\n",
      "  0.55530196 0.56541145 0.5161816  0.39653268 0.4606363  0.515031\n",
      "  0.58545226 0.44574842 0.44926447 0.49929667]\n",
      " [0.42084435 0.3536417  0.3806313  0.40856385 0.39491594 0.4184518\n",
      "  0.40248415 0.42165926 0.44574282 0.47248277 0.45661658 0.4001185\n",
      "  0.4656152  0.45883206 0.4893107  0.38238811]\n",
      " [0.5750165  0.6020194  0.64649177 0.5748234  0.5346377  0.4815687\n",
      "  0.48090726 0.4391494  0.4950905  0.645758   0.62138283 0.59045815\n",
      "  0.35480163 0.66357976 0.61248547 0.58270586]\n",
      " [0.535176   0.5428051  0.4548487  0.48162    0.5661198  0.58114743\n",
      "  0.60545456 0.614608   0.53912    0.3630197  0.45464516 0.5477911\n",
      "  0.6313925  0.4318106  0.42960486 0.5327021 ]\n",
      " [0.45177305 0.40415347 0.3833235  0.42121285 0.45068082 0.479413\n",
      "  0.47862178 0.5007422  0.47934103 0.40745658 0.4381706  0.4407478\n",
      "  0.5481684  0.42540148 0.45103118 0.4227747 ]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on some inputs\n",
    "reconstructed = autoencoder_forward(X_tensor[:5])\n",
    "print(\"Original:\\n\", X_tensor[:5].numpy())\n",
    "print(\"Reconstructed:\\n\", reconstructed.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f2cc9-f779-4f7a-bb2d-ffb5163a48a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
