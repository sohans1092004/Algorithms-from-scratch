{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79836e15-2ef7-49f6-9353-bd432a813391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    d_k = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "    scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores += (mask * -1e9)\n",
    "        \n",
    "    weights = tf.nn.softmax(scores, axis=-1)\n",
    "    return tf.matmul(weights, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f8aa59e-3b65-48e6-8b65-b7d352b244e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, Wq, Wk, Wv, Wo, num_heads):\n",
    "    batch_size = tf.shape(X)[0]\n",
    "    seq_len = tf.shape(X)[1]\n",
    "    d_model = X.shape[-1]\n",
    "    depth = d_model // num_heads\n",
    "\n",
    "    def split_heads(x):\n",
    "        x = tf.reshape(x, (batch_size, seq_len, num_heads, depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    Q = split_heads(tf.matmul(X, Wq))\n",
    "    K = split_heads(tf.matmul(X, Wk))\n",
    "    V = split_heads(tf.matmul(X, Wv))\n",
    "\n",
    "    attention_output = scaled_dot_product_attention(Q, K, V)\n",
    "    attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "    concat_attention = tf.reshape(attention_output, (batch_size, seq_len, d_model))\n",
    "    \n",
    "    return tf.matmul(concat_attention, Wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71903b8-4b5b-495d-bcd4-2628725bcfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_norm(x, sublayer_out, gamma, beta):\n",
    "    residual = x + sublayer_out\n",
    "    \n",
    "    mean = tf.reduce_mean(residual, axis=-1, keepdims=True)\n",
    "    var = tf.reduce_mean(tf.square(residual - mean), axis=-1, keepdims=True)\n",
    "    norm = (residual - mean) / tf.sqrt(var + 1e-6)\n",
    "    \n",
    "    return gamma * norm + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1ea9c0-d819-4e0f-a203-bbd0962e430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn(x, d_ff=128):\n",
    "    d_model = x.shape[-1]\n",
    "    \n",
    "    w1 = tf.Variable(tf.random.normal((d_model, d_ff)), trainable=True)\n",
    "    b1 = tf.Variable(tf.zeros([d_ff]), trainable=True)\n",
    "    \n",
    "    w2 = tf.Variable(tf.random.normal((d_ff, d_model)), trainable=True)\n",
    "    b2 = tf.Variable(tf.zeros([d_model]), trainable=True)\n",
    "    \n",
    "    return tf.matmul(tf.nn.relu(tf.matmul(x, w1) + b1), w2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ffccfd-c974-469a-a257-76f227ce6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_example(encoder_inputs, decoder_inputs, true_targets, all_vars):\n",
    "    optimizer = tf.optimizers.Adam(1e-3)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encoder\n",
    "        enc_input = encoder_inputs\n",
    "        enc_out = multi_head_attention(enc_input, Wq1, Wk1, Wv1, Wo1, num_heads)\n",
    "        enc_out = add_norm(enc_input, enc_out, gamma1, beta1)\n",
    "\n",
    "        # Decoder\n",
    "        dec_input = decoder_inputs\n",
    "        dec_out = multi_head_attention(dec_input, Wq2, Wk2, Wv2, Wo2, num_heads)\n",
    "        dec_out = add_norm(dec_input, dec_out, gamma2, beta2)\n",
    "\n",
    "        # prediction \n",
    "        logits = dec_out\n",
    "\n",
    "        # Loss\n",
    "        loss = tf.reduce_mean(tf.square(logits - true_targets))\n",
    "\n",
    "    grads = tape.gradient(loss, all_vars)\n",
    "    optimizer.apply_gradients(zip(grads, all_vars))\n",
    "\n",
    "    print(f\"Loss : {loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d0dfa5-d5b7-4648-977c-743c05104313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "\n",
    "# Dummy inputs and targets\n",
    "input_embeddings = tf.random.normal((batch_size, seq_len, d_model))\n",
    "input_pos = tf.random.normal((batch_size, seq_len, d_model))\n",
    "encoder_inputs = tf.add(input_embeddings, input_pos)\n",
    "\n",
    "target_embeddings = tf.random.normal((batch_size, seq_len, d_model))\n",
    "target_pos = tf.random.normal((batch_size, seq_len, d_model))\n",
    "decoder_inputs = tf.add(target_embeddings, target_pos)\n",
    "\n",
    "true_targets = tf.random.normal((batch_size, seq_len, d_model))\n",
    "\n",
    "# Initialize weights\n",
    "Wq1 = tf.Variable(tf.random.normal((d_model, d_model)), trainable=True)\n",
    "Wk1 = tf.Variable(tf.random.normal((d_model, d_model)), trainable=True)\n",
    "Wv1 = tf.Variable(tf.random.normal((d_model, d_model)), trainable=True)\n",
    "Wo1 = tf.Variable(tf.random.normal((d_model, d_model)), trainable=True)\n",
    "\n",
    "Wq2 = tf.Variable(tf.random.normal((d_model, d_model)), trainable=True)\n",
    "Wk2 = tf.Variable(tf.random.normal((d_model, d_model)), trainable=True)\n",
    "Wv2 = tf.Variable(tf.random.normal((d_model, d_model)), trainable=True)\n",
    "Wo2 = tf.Variable(tf.random.normal((d_model, d_model)), trainable=True)\n",
    "\n",
    "# gamma/beta for add_norm\n",
    "gamma1 = tf.Variable(tf.ones([d_model]), trainable=True)\n",
    "beta1 = tf.Variable(tf.zeros([d_model]), trainable=True)\n",
    "gamma2 = tf.Variable(tf.ones([d_model]), trainable=True)\n",
    "beta2 = tf.Variable(tf.zeros([d_model]), trainable=True)\n",
    "\n",
    "# All trainable variables\n",
    "all_vars = [Wq1, Wk1, Wv1, Wo1, Wq2, Wk2, Wv2, Wo2, gamma1, beta1, gamma2, beta2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6684413c-260f-4893-b081-19ffd901b149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramch\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.1840\n",
      "Loss : 2.1612\n",
      "Loss : 2.1306\n",
      "Loss : 2.0946\n",
      "Loss : 2.0700\n",
      "Loss : 2.0575\n",
      "Loss : 2.0472\n",
      "Loss : 2.0379\n",
      "Loss : 2.0266\n",
      "Loss : 2.0131\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "for _ in range(10):\n",
    "    run_training_example(encoder_inputs, decoder_inputs, true_targets, all_vars)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
